{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d265c2",
   "metadata": {},
   "source": [
    "Intro:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc25bf",
   "metadata": {},
   "source": [
    "## Data Loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb38d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "\n",
    "DATA_PATH = Path('../data/')\n",
    "ARTEFACTS_PATH = Path(\"../artefacts\")\n",
    "\n",
    "\n",
    "# Task 1 utilities:\n",
    "TASK_1_DATASET = DATA_PATH / Path(\"instacart-market-basket-analysis/data/instacart-market-basket-analysis\")\n",
    "TASK_1_ARTEFACTS_PATH = ARTEFACTS_PATH / Path('task_1')\n",
    "\n",
    "TASK_1_PATH_TO_CROSSTABLE = TASK_1_ARTEFACTS_PATH / Path(\"user_product_crosstable.npz\")\n",
    "TASK_1_PATH_TO_USER_IDS = TASK_1_ARTEFACTS_PATH / Path(\"user_ids.pkl\")\n",
    "TASK_1_PATH_TO_PRODUCT_IDS = TASK_1_ARTEFACTS_PATH / Path(\"product_ids.pkl\")\n",
    "\n",
    "TASK_1_PATH_TO_CROSSTABLE_VAL = TASK_1_ARTEFACTS_PATH / Path(\"user_product_crosstable_val.npz\")\n",
    "TASK_1_PATH_TO_USER_IDS_VAL = TASK_1_ARTEFACTS_PATH / Path(\"user_ids_val.pkl\")\n",
    "TASK_1_PATH_TO_PRODUCT_IDS_VAL = TASK_1_ARTEFACTS_PATH / Path(\"product_ids_val.pkl\")\n",
    "\n",
    "TASK_1_PATH_TO_USER_VECTORS = TASK_1_ARTEFACTS_PATH / Path(\"svd/user_vectors.pkl\")\n",
    "TASK_1_PATH_TO_PRODUCT_VECTORS = TASK_1_ARTEFACTS_PATH / Path(\"svd/product_vectors.pkl\")\n",
    "TASK_1_PATH_TO_SIGMA_VECTORS = TASK_1_ARTEFACTS_PATH / Path(\"svd/sigma_vectors.pkl\")\n",
    "\n",
    "\n",
    "TASK_1_PATH_TO_USER_VECTORS_VAL = TASK_1_ARTEFACTS_PATH / Path(\"svd/user_vectors_val.pkl\")\n",
    "TASK_1_PATH_TO_PRODUCT_VECTORS_VAL = TASK_1_ARTEFACTS_PATH / Path(\"svd/product_vectors_val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db8373a",
   "metadata": {},
   "source": [
    "## Data preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d7f6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_train_and_test_orders():\n",
    "\n",
    "    # 'Products' datasets:\n",
    "    products = pd.read_csv(TASK_1_DATASET / 'products.csv/products.csv')\n",
    "    aisles = pd.read_csv(TASK_1_DATASET / 'aisles.csv/aisles.csv')\n",
    "    departments = pd.read_csv(TASK_1_DATASET / 'departments.csv/departments.csv')\n",
    "\n",
    "    # Orders datasets:\n",
    "    orders = pd.read_csv(TASK_1_DATASET / 'orders.csv/orders.csv')\n",
    "    orderProductsTrain = pd.read_csv(TASK_1_DATASET / 'order_products__train.csv/order_products__train.csv')\n",
    "    orderProductsPrior = pd.read_csv(TASK_1_DATASET / 'order_products__prior.csv/order_products__prior.csv')\n",
    "\n",
    "    goods = add_departments_and_aisles_info(products=products, departments=departments, aisles=aisles)\n",
    "\n",
    "    orders_detailed_train = combine_orders_info(orders=orders, orders_products=orderProductsTrain)\n",
    "    orders_detailed_train = merge_orders_and_goods(order_detailed=orders_detailed_train, goods=goods)\n",
    "\n",
    "    orders_detailed_prior = combine_orders_info(orders=orders, orders_products=orderProductsPrior)\n",
    "    orders_detailed_prior = merge_orders_and_goods(order_detailed=orders_detailed_prior, goods=goods)\n",
    "\n",
    "    orders_detailed = concat_prior_and_train_orders(\n",
    "        order_detailed_train=orders_detailed_train, order_detailed_prior=orders_detailed_prior\n",
    "    )\n",
    "    return orders_detailed, orders.query(\"eval_set == 'test'\")\n",
    "\n",
    "\n",
    "def add_departments_and_aisles_info(products: pd.DataFrame, departments: pd.DataFrame, aisles: pd.DataFrame):\n",
    "    # combine aisles, departments and products (left joined to products)\n",
    "    goods = pd.merge(\n",
    "        left=pd.merge(\n",
    "            left=products, right=departments, how='left'\n",
    "        ),\n",
    "        right=aisles,\n",
    "        how='left'\n",
    "    )\n",
    "    # to retain '-' and make product names more \"standard\"\n",
    "    goods.product_name = goods.product_name.str.replace(' ', '_').str.lower()\n",
    "    return goods\n",
    "\n",
    "\n",
    "def combine_orders_info(orders: pd.DataFrame, orders_products: pd.DataFrame):\n",
    "    # initialize it with train dataset\n",
    "    order_details = pd.merge(\n",
    "        left=orders_products,\n",
    "        right=orders,\n",
    "        how='left',\n",
    "        on='order_id'\n",
    "    ).apply(partial(pd.to_numeric, errors='ignore', downcast='integer'))\n",
    "    return order_details\n",
    "\n",
    "\n",
    "def merge_orders_and_goods(order_detailed: pd.DataFrame, goods: pd.DataFrame):\n",
    "    # add order hierarchy\n",
    "    order_detailed = pd.merge(\n",
    "        left=order_detailed.copy(),\n",
    "        right=goods[['product_id',\n",
    "                     'aisle_id',\n",
    "                     'department_id']].apply(partial(pd.to_numeric,\n",
    "                                                     errors='ignore',\n",
    "                                                     downcast='integer')),\n",
    "        how='left',\n",
    "        on='product_id'\n",
    "    )\n",
    "    return order_detailed\n",
    "\n",
    "\n",
    "def concat_prior_and_train_orders(order_detailed_train: pd.DataFrame, order_detailed_prior: pd.DataFrame):\n",
    "    order_detailed_train = order_detailed_train.copy()\n",
    "    indexes = np.linspace(0, len(order_detailed_prior), num=10, dtype=np.int32)\n",
    "\n",
    "    for i in range(len(indexes) - 1):\n",
    "        order_detailed_train = pd.concat([order_detailed_train, order_detailed_prior.iloc[indexes[i]:indexes[i+1], :]])\n",
    "    return order_detailed_train\n",
    "\n",
    "\n",
    "def get_last_order_for_users(order_detailed: pd.DataFrame) -> pd.DataFrame:\n",
    "    mask = order_detailed.groupby(\"user_id\")[\"order_number\"].transform(max) == order_detailed['order_number']\n",
    "    last_orders = order_detailed.loc[mask]\n",
    "    return last_orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "265a7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_detailed, orders_detailed_test = get_train_and_test_orders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d0f8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>add_to_cart_order</th>\n",
       "      <th>reordered</th>\n",
       "      <th>user_id</th>\n",
       "      <th>eval_set</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>department_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>49302</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>112108</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>120</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11109</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>112108</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>108</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10246</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>112108</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>49683</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>112108</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>43633</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>112108</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>95</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id  product_id  add_to_cart_order  reordered  user_id eval_set  \\\n",
       "0         1       49302                  1          1   112108    train   \n",
       "1         1       11109                  2          1   112108    train   \n",
       "2         1       10246                  3          0   112108    train   \n",
       "3         1       49683                  4          0   112108    train   \n",
       "4         1       43633                  5          1   112108    train   \n",
       "\n",
       "   order_number  order_dow  order_hour_of_day  days_since_prior_order  \\\n",
       "0             4          4                 10                     9.0   \n",
       "1             4          4                 10                     9.0   \n",
       "2             4          4                 10                     9.0   \n",
       "3             4          4                 10                     9.0   \n",
       "4             4          4                 10                     9.0   \n",
       "\n",
       "   aisle_id  department_id  \n",
       "0       120             16  \n",
       "1       108             16  \n",
       "2        83              4  \n",
       "3        83              4  \n",
       "4        95             15  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_detailed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f071ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>eval_set</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2774568</td>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>329954</td>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1528013</td>\n",
       "      <td>6</td>\n",
       "      <td>test</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1376945</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1356845</td>\n",
       "      <td>12</td>\n",
       "      <td>test</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     order_id  user_id eval_set  order_number  order_dow  order_hour_of_day  \\\n",
       "38    2774568        3     test            13          5                 15   \n",
       "44     329954        4     test             6          3                 12   \n",
       "53    1528013        6     test             4          3                 16   \n",
       "96    1376945       11     test             8          6                 11   \n",
       "102   1356845       12     test             6          1                 20   \n",
       "\n",
       "     days_since_prior_order  \n",
       "38                     11.0  \n",
       "44                     30.0  \n",
       "53                     22.0  \n",
       "96                      8.0  \n",
       "102                    30.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_detailed_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6360f5",
   "metadata": {},
   "source": [
    "# Split dataset into train and valid:  \n",
    "Normally, that should be implemented by time-series split. But since there are no timestamps, lets take 'prior' dataset as our train and the 'train' dataset as validation.  \n",
    "Moreover, let's keep the same set of items in both new subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a0548bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'split':\n",
    "train_set = orders_detailed.query(\"eval_set == 'prior'\")\n",
    "val_set = orders_detailed.query(\"eval_set == 'train'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12daea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep products only from train:\n",
    "val_set = val_set.query(\"product_id in @train_set.product_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2ce2b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32434489, 12), (1384608, 12))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape, val_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7733b4f",
   "metadata": {},
   "source": [
    "# Vectors generation pipeline:\n",
    "1. Create crosstab of USERxPRODUCT pairs; binary counts\n",
    "2. SVD of it\n",
    "3. Use vectors as USER and PRODUCT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05e78653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_artefact(artefact, artefact_path: Path):\n",
    "    with open(artefact_path, 'wb') as f:\n",
    "        pickle.dump(artefact, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e62bb1",
   "metadata": {},
   "source": [
    "## Create crosstabs (uncomment if want to regenerate):  \n",
    "The idea is in simulation that the data is recieved in the time-series manner:\n",
    "- Firstly the `train` set was avaliable, the crosstab created on that data\n",
    "- Then new data (`valid` set) was avaliable, so the new crosstab will be created on the train+validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75c191a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.contingency import crosstab\n",
    "\n",
    "\n",
    "def create_crosstab_artefacts(dataset: pd.DataFrame, crosstab_path, user_ids_path, product_ids_path):\n",
    "\n",
    "    (user_ids, product_ids), X = crosstab(\n",
    "        dataset[\"user_id\"].values, dataset[\"product_id\"].values, sparse=True\n",
    "    )\n",
    "\n",
    "    # To binary representation of counts:\n",
    "    X = X != 0\n",
    "    X = X.astype('int')\n",
    "    \n",
    "    # Save:\n",
    "    paths = [crosstab_path, user_ids_path, product_ids_path]\n",
    "    objs = [X, user_ids, product_ids]\n",
    "    for path, obj in zip(paths, objs):\n",
    "        save_artefact(artefact=obj, artefact_path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97479578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create crosstab and IDs for train subset:\n",
    "create_crosstab_artefacts(train_set, crosstab_path=TASK_1_PATH_TO_CROSSTABLE, user_ids_path=TASK_1_PATH_TO_USER_IDS, product_ids_path=TASK_1_PATH_TO_PRODUCT_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67dd798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create crosstab and IDs for train+val subset:\n",
    "create_crosstab_artefacts(train_set.append(val_set), crosstab_path=TASK_1_PATH_TO_CROSSTABLE_VAL, user_ids_path=TASK_1_PATH_TO_USER_IDS_VAL, product_ids_path=TASK_1_PATH_TO_PRODUCT_IDS_VAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53910b19",
   "metadata": {},
   "source": [
    "## Load crosstab artefacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15be6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_crosstab_artefacts(path_to_crosstable: Path, user_ids_filepath: Path, product_ids_filepath: Path):\n",
    "    if path_to_crosstable.exists() and user_ids_filepath.exists() and product_ids_filepath.exists():\n",
    "        return load_pickled(path_to_crosstable), load_pickled(user_ids_filepath), load_pickled(product_ids_filepath)\n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "def load_pickled(filepath: Path):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "    \n",
    "X_train, user_ids_train, product_ids_train = load_crosstab_artefacts(\n",
    "    TASK_1_PATH_TO_CROSSTABLE, \n",
    "    TASK_1_PATH_TO_USER_IDS,\n",
    "    TASK_1_PATH_TO_PRODUCT_IDS\n",
    ")\n",
    "\n",
    "\n",
    "X_val, user_ids_val, product_ids_val = load_crosstab_artefacts(\n",
    "    TASK_1_PATH_TO_CROSSTABLE_VAL, \n",
    "    TASK_1_PATH_TO_USER_IDS_VAL,\n",
    "    TASK_1_PATH_TO_PRODUCT_IDS_VAL\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209595f6",
   "metadata": {},
   "source": [
    "## Generate SVD vectors (uncomment if want to regenerate):  \n",
    "- Firstly the `train` set was avaliable, the crosstab created on that data, so SVD will be performed on that. These vectors will be used in model's training\n",
    "- Then new data (`valid` set) was avaliable, so the new crosstab will be created on the train+validation data and now SVD will be performed on that also. These vectors will be used in model's validation (only for validation subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6f5c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "\n",
    "\n",
    "def generate_svd_vectors(crosstab, n_components: int, user_vectors_path, product_vectors_path):\n",
    "\n",
    "    u, _, v = randomized_svd(crosstab, n_components=n_components)\n",
    "    \n",
    "    paths = [user_vectors_path, product_vectors_path]\n",
    "    objs = [u, v]\n",
    "    for path, obj in zip(paths, objs):\n",
    "        save_artefact(artefact=obj, artefact_path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1256859",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 500\n",
    "\n",
    "generate_svd_vectors(X_train, n_components=n_components, user_vectors_path=TASK_1_PATH_TO_USER_VECTORS, product_vectors_path=TASK_1_PATH_TO_PRODUCT_VECTORS)\n",
    "generate_svd_vectors(X_val, n_components=n_components, user_vectors_path=TASK_1_PATH_TO_USER_VECTORS_VAL, product_vectors_path=TASK_1_PATH_TO_PRODUCT_VECTORS_VAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046090b",
   "metadata": {},
   "source": [
    "# Neural network pipeline:  \n",
    "1. For TRAIN subset: get user vectors, get product vectors\n",
    "2. Predict whether reordered or not\n",
    "3. Negative sampling?\n",
    "4. Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f91c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train vectors: \n",
    "with open(TASK_1_PATH_TO_USER_VECTORS, 'rb') as f:\n",
    "    user_vectors = pickle.load(f)\n",
    "    \n",
    "with open(TASK_1_PATH_TO_PRODUCT_VECTORS, 'rb') as f:\n",
    "    product_vectors = pickle.load(f)\n",
    "    \n",
    "    \n",
    "# Load valid vectors: \n",
    "with open(TASK_1_PATH_TO_USER_VECTORS_VAL, 'rb') as f:\n",
    "    user_vectors_val = pickle.load(f)\n",
    "    \n",
    "with open(TASK_1_PATH_TO_PRODUCT_VECTORS_VAL, 'rb') as f:\n",
    "    product_vectors_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d475ba7",
   "metadata": {},
   "source": [
    "## Dataset init:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe89309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class UserProductDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataframe: pd.DataFrame,\n",
    "        user_vectors: np.ndarray, \n",
    "        product_vectors: np.ndarray,\n",
    "        user_ids: np.ndarray, \n",
    "        product_ids: np.ndarray\n",
    "    ):\n",
    "        self.order_id_samples = dataframe['order_id'].tolist()\n",
    "        self.user_id_samples = dataframe['user_id'].tolist()\n",
    "        self.product_id_samples = dataframe['product_id'].tolist()\n",
    "        self.labels = dataframe['reordered'].tolist()\n",
    "        \n",
    "        self.user_ids = user_ids\n",
    "        self.product_ids = product_ids\n",
    "        \n",
    "        self.user_id_to_idx = {id_: idx for idx, id_ in enumerate(self.user_ids)}\n",
    "        self.product_id_to_idx = {id_: idx for idx, id_ in enumerate(self.product_ids)}\n",
    "        \n",
    "        self.user_vectors = user_vectors\n",
    "        if product_vectors.shape[0] == user_vectors.shape[1]:\n",
    "            self.product_vectors = product_vectors.T\n",
    "        else:\n",
    "            self.product_vectors = product_vectors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.order_id_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id = self.user_id_samples[idx]\n",
    "        product_id = self.product_id_samples[idx]\n",
    "        \n",
    "        user_idx = self.user_id_to_idx[user_id]\n",
    "        product_idx = self.product_id_to_idx[product_id]\n",
    "        \n",
    "        user_vector = self.user_vectors[user_idx]\n",
    "        product_vector = self.product_vectors[product_idx]\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return user_vector, product_vector, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "024436f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>add_to_cart_order</th>\n",
       "      <th>reordered</th>\n",
       "      <th>user_id</th>\n",
       "      <th>eval_set</th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>department_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>49302</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>112108</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>120</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11109</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>112108</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>108</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10246</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>112108</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>49683</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>112108</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>43633</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>112108</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>9.0</td>\n",
       "      <td>95</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384612</th>\n",
       "      <td>3421063</td>\n",
       "      <td>14233</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>169679</td>\n",
       "      <td>train</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>115</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384613</th>\n",
       "      <td>3421063</td>\n",
       "      <td>35548</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>169679</td>\n",
       "      <td>train</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384614</th>\n",
       "      <td>3421070</td>\n",
       "      <td>35951</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>139822</td>\n",
       "      <td>train</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8.0</td>\n",
       "      <td>91</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384615</th>\n",
       "      <td>3421070</td>\n",
       "      <td>16953</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>139822</td>\n",
       "      <td>train</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8.0</td>\n",
       "      <td>88</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384616</th>\n",
       "      <td>3421070</td>\n",
       "      <td>4724</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>139822</td>\n",
       "      <td>train</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1384608 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         order_id  product_id  add_to_cart_order  reordered  user_id eval_set  \\\n",
       "0               1       49302                  1          1   112108    train   \n",
       "1               1       11109                  2          1   112108    train   \n",
       "2               1       10246                  3          0   112108    train   \n",
       "3               1       49683                  4          0   112108    train   \n",
       "4               1       43633                  5          1   112108    train   \n",
       "...           ...         ...                ...        ...      ...      ...   \n",
       "1384612   3421063       14233                  3          1   169679    train   \n",
       "1384613   3421063       35548                  4          1   169679    train   \n",
       "1384614   3421070       35951                  1          1   139822    train   \n",
       "1384615   3421070       16953                  2          1   139822    train   \n",
       "1384616   3421070        4724                  3          1   139822    train   \n",
       "\n",
       "         order_number  order_dow  order_hour_of_day  days_since_prior_order  \\\n",
       "0                   4          4                 10                     9.0   \n",
       "1                   4          4                 10                     9.0   \n",
       "2                   4          4                 10                     9.0   \n",
       "3                   4          4                 10                     9.0   \n",
       "4                   4          4                 10                     9.0   \n",
       "...               ...        ...                ...                     ...   \n",
       "1384612            30          0                 10                     4.0   \n",
       "1384613            30          0                 10                     4.0   \n",
       "1384614            15          6                 10                     8.0   \n",
       "1384615            15          6                 10                     8.0   \n",
       "1384616            15          6                 10                     8.0   \n",
       "\n",
       "         aisle_id  department_id  \n",
       "0             120             16  \n",
       "1             108             16  \n",
       "2              83              4  \n",
       "3              83              4  \n",
       "4              95             15  \n",
       "...           ...            ...  \n",
       "1384612       115              7  \n",
       "1384613        13             20  \n",
       "1384614        91             16  \n",
       "1384615        88             13  \n",
       "1384616        32              4  \n",
       "\n",
       "[1384608 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a26c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = UserProductDataset(\n",
    "    dataframe=train_set,\n",
    "    user_vectors=user_vectors,\n",
    "    product_vectors=product_vectors,\n",
    "    user_ids=user_ids_train,\n",
    "    product_ids=product_ids_train\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = UserProductDataset(\n",
    "    dataframe=val_set,\n",
    "    user_vectors=user_vectors_val,\n",
    "    product_vectors=product_vectors_val,\n",
    "    user_ids=user_ids_val,\n",
    "    product_ids=product_ids_val\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136dcc72",
   "metadata": {},
   "source": [
    "## Networks init:\n",
    "1. Cosine Similarity as output\n",
    "2. Softmax as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e0fcf697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CosineRecommender(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CosineRecommender, self).__init__()\n",
    "        self.user_fc = nn.Linear(input_size, hidden_size)  \n",
    "        self.product_fc = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.CosineSimilarity()\n",
    "\n",
    "    def forward(self, users_inputs, products_input):\n",
    "        uv = self.relu(self.user_fc(users_inputs))\n",
    "        pv = self.relu(self.product_fc(products_input))\n",
    "        out = self.output(uv, pv)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class SigmoidRecommender(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SigmoidRecommender, self).__init__()\n",
    "        self.user_fc = nn.Linear(input_size, hidden_size) \n",
    "        self.product_fc = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(hidden_size*2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, users_inputs, products_input):\n",
    "        uv = self.relu(self.user_fc(users_inputs))\n",
    "        pv = self.relu(self.product_fc(products_input))\n",
    "        x = torch.cat([uv, pv], dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x).squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4fc98",
   "metadata": {},
   "source": [
    "## Training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f534e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Train function:\n",
    "def train_on_train(model_class, n_epochs: int, input_size: int=500, hidden_size: int=256, output_size: int=1):\n",
    "\n",
    "    model = model_class(input_size, hidden_size, output_size)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for _ in range(n_epochs):\n",
    "        for user_vectors, product_vectors, labels in tqdm(train_dataloader):\n",
    "            # Convert inputs and labels to tensors\n",
    "            user_vectors = torch.tensor(user_vectors).float()\n",
    "            product_vectors = torch.tensor(product_vectors).float()\n",
    "            labels = torch.tensor(labels).float()\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(user_vectors, product_vectors)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Predict function:\n",
    "def predict_on_val(model, val_dataloader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        predictions = []\n",
    "        prodictions_sims = []\n",
    "        for user_vectors, product_vectors, labels in tqdm(val_dataloader):\n",
    "        \n",
    "            user_vectors = torch.tensor(user_vectors).float()\n",
    "            product_vectors = torch.tensor(product_vectors).float()\n",
    "            labels = torch.tensor(labels).float()\n",
    "            \n",
    "            logits = model(user_vectors, product_vectors)\n",
    "            outputs = (logits > 0.5).int()\n",
    "            outputs = outputs.tolist()\n",
    "            predictions.extend(outputs)\n",
    "            prodictions_sims.extend(logits.tolist())\n",
    "    return predictions, prodictions_sims\n",
    "\n",
    "\n",
    "def calculate_f1(y_true, y_pred):\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return f1\n",
    "\n",
    "\n",
    "# MAP@K:\n",
    "def precision_at_k(y_true, y_pred, k=5):\n",
    "    intersection = set(y_pred[:k]) & set(y_true)\n",
    "    return len(intersection) / k\n",
    "\n",
    "\n",
    "def average_precision_at_k(y_true, y_pred, k=5):\n",
    "    hits = 0\n",
    "    s = 0\n",
    "    for i in range(len(y_pred[:k])):\n",
    "        if y_pred[i] in y_true:\n",
    "            hits += 1\n",
    "            s += hits / (i + 1)\n",
    "    if hits == 0:\n",
    "        return 0\n",
    "    return s / hits\n",
    "\n",
    "\n",
    "def map_at_k(y_true, y_pred, k=5):\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    return np.mean([\n",
    "        average_precision_at_k(y_true[i], y_pred[i], k=k)\n",
    "        for i in range(len(y_true))\n",
    "    ])\n",
    "\n",
    "\n",
    "# NDCG:\n",
    "def ndcg_at_k(y_true, y_pred, k=5):\n",
    "    ideal_gain = sum([1 / np.log2(i + 2) for i in range(k)])\n",
    "    dcg = sum([\n",
    "        1 / np.log2(i + 2)\n",
    "        for i, rating in enumerate(y_pred[:k])\n",
    "        if rating in y_true\n",
    "    ])\n",
    "    return dcg / ideal_gain\n",
    "\n",
    "\n",
    "def NDCG_at_k(y_true, y_pred, k=5):\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    return np.mean([\n",
    "        ndcg_at_k(y_true[i], y_pred[i], k=k)\n",
    "        for i in range(len(y_true))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c556ac",
   "metadata": {},
   "source": [
    "### Cosine Similarity Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8fb420d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for user_vectors, product_vectors, labels in tqdm(train_dataloader):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4a6a52aadc44b3874666f4f5eb8e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/253395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  user_vectors = torch.tensor(user_vectors).float()\n",
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  product_vectors = torch.tensor(product_vectors).float()\n",
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).float()\n",
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for user_vectors, product_vectors, labels in tqdm(val_dataloader):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3ddc3a486f4be2b6b2f960ebed2557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  user_vectors = torch.tensor(user_vectors).float()\n",
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  product_vectors = torch.tensor(product_vectors).float()\n",
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).float()\n"
     ]
    }
   ],
   "source": [
    "f1_scores = []\n",
    "mapk_scores = []\n",
    "ndcgk_scores = []\n",
    "\n",
    "# n_epochs = [1, 3, 5, 10]  # too slow\n",
    "n_epochs = [1,]\n",
    "\n",
    "K = 10\n",
    "\n",
    "\n",
    "for n_epoch in n_epochs:\n",
    "    cosine_model = CosineRecommender\n",
    "    cosine_model = train_on_train(cosine_model, n_epochs=n_epoch)\n",
    "\n",
    "    cosine_preds, cosine_sims = predict_on_val(cosine_model, val_dataloader)\n",
    "    f1 = calculate_f1(val_set.reordered, cosine_preds)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    \n",
    "    # Prepare outputs for MAP@k and other rank-based metrics:\n",
    "    val_set['cosine_sims'] = cosine_sims\n",
    "    grouped = val_set.groupby(\"order_id\")\n",
    "    \n",
    "    cosine_sims = grouped['cosine_sims'].apply(np.array).values\n",
    "    cosine_sims = [val.tolist() for val in cosine_sims]\n",
    "\n",
    "    reordered = grouped['reordered'].apply(np.array).values\n",
    "    reordered = [val.tolist() for val in reordered]\n",
    "    \n",
    "    for i in range(len(cosine_sims)):\n",
    "        reordered[i], cosine_sims[i] = zip(*sorted(zip(reordered[i], cosine_sims[i]), key=lambda x: x[1]))\n",
    "        \n",
    "    mapk = map_at_k(reordered, cosine_sims, k=K)\n",
    "    ndcgk = NDCG_at_k(reordered, cosine_sims, k=K)\n",
    "    \n",
    "    mapk_scores.append(mapk)\n",
    "    ndcgk_scores.append(ndcgk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff50744a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.333, MAP@K = 0.71, NDCG@K = 0.335, K=10, n_epochs = 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mapk_scores)):\n",
    "    print(f\"F1 = {round(f1_scores[i], 3)}, MAP@K = {round(mapk_scores[i], 3)}, NDCG@K = {round(ndcgk_scores[i], 3)}, K={K}, n_epochs = {n_epochs[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a107b1",
   "metadata": {},
   "source": [
    "### Sigmoid model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f6a08c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for user_vectors, product_vectors, labels in tqdm(train_dataloader):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a948149b430f49079bd25fcceb4017ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/253395 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  user_vectors = torch.tensor(user_vectors).float()\n",
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  product_vectors = torch.tensor(product_vectors).float()\n",
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).float()\n",
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for user_vectors, product_vectors, labels in tqdm(val_dataloader):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101c2a96093640268985063d51725249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  user_vectors = torch.tensor(user_vectors).float()\n",
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  product_vectors = torch.tensor(product_vectors).float()\n",
      "/var/folders/p0/qqqfskpd1v1fyxk0_v46k07r0000gp/T/ipykernel_7764/3230568664.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).float()\n"
     ]
    }
   ],
   "source": [
    "f1_scores = []\n",
    "mapk_scores = []\n",
    "ndcgk_scores = []\n",
    "\n",
    "\n",
    "# n_epochs = [1, 3, 5, 10]  # too slow\n",
    "n_epochs = [1,]\n",
    "\n",
    "K = 10\n",
    "\n",
    "\n",
    "for n_epoch in n_epochs:\n",
    "    model = SigmoidRecommender\n",
    "    model = train_on_train(model, n_epochs=n_epoch)\n",
    "\n",
    "    preds, cosine_sims = predict_on_val(model, val_dataloader)\n",
    "    f1 = calculate_f1(val_set.reordered, preds)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    \n",
    "    # Prepare outputs for MAP@k and other rank-based metrics:\n",
    "    val_set['cosine_sims'] = cosine_sims\n",
    "    grouped = val_set.groupby(\"order_id\")\n",
    "    \n",
    "    cosine_sims = grouped['cosine_sims'].apply(np.array).values\n",
    "    cosine_sims = [val.tolist() for val in cosine_sims]\n",
    "\n",
    "    reordered = grouped['reordered'].apply(np.array).values\n",
    "    reordered = [val.tolist() for val in reordered]\n",
    "    \n",
    "    for i in range(len(cosine_sims)):\n",
    "        reordered[i], cosine_sims[i] = zip(*sorted(zip(reordered[i], cosine_sims[i]), key=lambda x: x[1]))\n",
    "        \n",
    "    mapk = map_at_k(reordered, cosine_sims, k=K)\n",
    "    ndcgk = NDCG_at_k(reordered, cosine_sims, k=K)\n",
    "    \n",
    "    mapk_scores.append(mapk)\n",
    "    ndcgk_scores.append(ndcgk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "31b43392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.358, MAP@K = 0.09, NDCG@K = 0.021, K=10, n_epochs = 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mapk_scores)):\n",
    "    print(f\"F1 = {round(f1_scores[i], 3)}, MAP@K = {round(mapk_scores[i], 3)}, NDCG@K = {round(ndcgk_scores[i], 3)}, K={K}, n_epochs = {n_epochs[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "742305c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.405, MAP@K = 0.057, NDCG@K = 0.023, K=10, n_epochs = 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mapk_scores)):\n",
    "    print(f\"F1 = {round(f1_scores[i], 3)}, MAP@K = {round(mapk_scores[i], 3)}, NDCG@K = {round(ndcgk_scores[i], 3)}, K={K}, n_epochs = {n_epochs[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
